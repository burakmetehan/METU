\section{Central Limit Theorem}

We now turn our attention to sums of random variables,
\begin{equation*}
    S_n = X_1 + ... + X_n,
\end{equation*}
that appear in many applications. Let $\mu = \expc{X_i}$ and $\sigma = \textnormal{Std}(X_i)$ for all $i = 1, ..., n$. How does $S_n$ behave for large $n$?

\begin{itemize}
    \item The pure sum $S_n$ diverges. In fact, this should be anticipated because
    \begin{equation*}
        \var{S_n} = n\sigma^2 \rightarrow \infty
    \end{equation*}
    so that variability of $S_n$ grows unboundedly as $n$ goes to infinity.
    \item The average $\dfrac{S_n}{n}$ converges. Indeed, in this case, we have
    \begin{equation*}
        \var{\frac{S_n}{n}} = \frac{\var{S_n}}{n^2} = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n} \rightarrow 0
    \end{equation*}
    so that variability of $\dfrac{S_n}{n}$ vanishes as $n \to \infty$.
    \item An interesting normalization factor is $\dfrac{1}{\sqrt[]{n}}$. For $\mu = 0$, $\dfrac{S_n}{\sqrt{n}}$ \textit{neither diverges nor converges}! It does not tend to leave 0, but it does not converge to 0 either. Rather, it behaves like some random variable. The following theorem states that this variable has approximately Normal distribution for large $n$.
\end{itemize}

\begin{theorem}{: Central Limit Theorem}
    Let $X_1, X_2, ...$ be independent random variables with the same expectation $\mu = \expc{X_i}$ and the same standard deviation $\sigma = \textnormal{Std}(X_i)$, and let
    \begin{equation*}
        S_n = \sum_{i=1}^{n} X_i = X_i + ... + X_n
    \end{equation*}
    As $n \to \infty$, the standardized sum
    \begin{equation*}
        Z_n = \frac{S_n - \expc{S_n}}{\textnormal{Std}(S_n)} = \frac{S_n - n\mu}{\sigma\sqrt{n}}
    \end{equation*}
    converges in distribution to a Standard Normal random variable, that is,
    \begin{equation}
        F_{Z_n}(z) = \prob{\frac{S_n-n\mu}{\sigma\sqrt{n}} \leq z} \to \Phi(z)
    \end{equation}
    for all $z$.
\end{theorem}

This theorem is very powerful because it can be applied to random variables $X_1, X_2, ...$ having virtually any thinkable distribution with finite expectation and variance. As long as $n$ is large (the rule of thumb is $n > 30$), one can use Normal distribution to compute probabilities about $S_n$.

Theorem 1 is only one basic version of the \texttt{Central Limit Theorem}. Over the last two centuries, it has been extended to large classes of dependent variables and vectors, stochastic processes, and so on.

\begin{example}{: Allocation of disk space}
    A disk has free space of 330 megabytes. Is it likely to be sufficient for 300 independent images, if each image has expected size of 1 megabyte with a standard deviation of 0.5 megabytes?\\

    \textbf{Solution:} We have $n = 300$, $\mu = 1$, $\sigma = 0.5$. The number of images $n$ is large, so the Central Limit Theorem applies to their total size $S_n$. Then,
    \begin{align*}
        \prob{\textnormal{sufficient space}} &= \prob{S_n \leq 330} = \prob{\frac{S_n - n\mu}{\sigma \sqrt{n}} \leq \frac{330-(300)(1)}{0.5\sqrt{300}}}\\
        &\approx \Phi(3.46) = 0.9997
    \end{align*}
    This probability is very high, hence, the available disk space is very likely to be sufficient.
\end{example}

In the special case of Normal variables $X_1, X_2, ...$, the distribution of $S_n$ is always Normal, and (eq. 18) becomes exact equality for arbitrary, even small $n$.

\begin{example}{: Elevator}
    You wait for an elevator, whose capacity is 2000 pounds. The elevator comes with ten adult passengers. Suppose your own weight is 150 lbs, and you heard that human weights are normally distributed with the mean of 165 lbs and the standard deviation of 20 lbs. Would you board this elevator or wait for the next one?\\

    \textbf{Solution:}
    In other words, is overload likely? The probability of an overload equals 
    \begin{align*}
        \prob{S_{10} + 150 > 2000} &= \prob{\frac{S_10-(10)(165)}{20\sqrt{10}} > \frac{2000-150-(10)(165)}{20\sqrt{10}}}\\
        &= 1 - \Phi(3.16) = 0.0008
    \end{align*}
    So, with probability 0.9992 it is safe to take this elevator. It is now for you to decide.
\end{example}

Among the random variables discussed in Chapters 3 and 4, at least three have a form of $S_n$:
\begin{align*}
    &\textnormal{Binomial variable} &= &&&\textnormal{sum of independent Bernoulli variables}\\
    &\textnormal{Negative Binomial variable} &= &&&\textnormal{sum of independent Geometric variables}\\
    &\textnormal{Gamma variable} &= &&&\textnormal{sum of independent Exponential variables}
\end{align*}
Hence, the Central Limit Theorem applies to all these distributions with sufficiently large $n$ in the case of Binomial, $k$ for Negative Binomial, and $\alpha$ for Gamma variables.

\subsection{Normal Approximation to Binomial Distribution}

Binomial variables represent a special case of $S_n = X_1 + ... + X_n$, where all $X_i$ have Bernoulli distribution with some parameter $p$. We know that small $p$ allows to approximate Binomial distribution with Poisson, and large $p$ allows such an approximation for the number of failures. For the moderate values of $p$ (say, $0.05 \leq p \leq 0.95$) and for large $n$, we can use Theorem 1:
\begin{equation}
    \textnormal{Binomial}(n, p) \approx Normal \left( \mu = np, \sigma = \sqrt{np(1-p)} \right)
\end{equation}

\subsection{Continuity Correction}

This correction is needed when we approximate a discrete distribution (Binomial in this case) by a continuous distribution (Normal). Recall that the probability $\prob{X = x}$ may be positive if $X$ is discrete, whereas it is always 0 for continuous $X$. Thus, a direct use of (eq. 19) will always approximate this probability by 0. It is obviously a poor approximation.

This is resolved by introducing a \textbf{\textit{continuity correction}}. Expand the interval by 0.5 units in each direction, then use the Normal approximation. Notice that
\begin{equation*}
    P_X(x) = \prob{X = x} = \prob{x - 0.5 < X < x + 0.5}
\end{equation*}
is true for a Binomial variable $X$; therefore, the continuity correction does not change the event and preserves its probability. It makes a difference for the Normal distribution, so every time when we approximate some discrete distribution with some continuous distribution, we should be using a continuity correction. Now it is the probability of an interval instead of one number, and it is not zero.

\begin{example}{}
    A new computer virus attacks a folder consisting of 200 files. Each file gets damaged with probability 0.2 independently of other files. What is the probability that fewer than 50 files get damaged?\\

    \textbf{Solution:} The number $X$ of damaged files has Binomial distribution with $n = 200$, $p = 0.2$, $\mu = np = 40$, and $\sigma = \sqrt{np(1-p)} = 5.657$. Applying the Central Limit Theorem with the continuity correction,
    \begin{align*}
        \prob{X < 50} &= \prob{X < 49.5} = \prob{\frac{X-40}{5.657} < \frac{49.5-40}{5.657}}\\
        &= \Phi(1.68) = 0.9535
    \end{align*}
    Notice that the properly applied continuity correction replaces 50 with 49.5, not 50.5. Indeed, we are interested in the event that $X$ is strictly less than 50. This includes all values up to 49 and corresponds to the interval $\left[ 0, 49 \right]$ that we expand to $\left[ 0, 49.5 \right]$. In other words, events $\{X < 50\}$ and $\{X < 49.5\}$ are the same; they include the same possible values of $X$. Events $\{X < 50\}$ and $\{X < 50.5\}$ are different because the former includes $X = 50$, and the latter does not. Replacing $\{X < 50\}$ with $\{X < 50.5\}$ would have changed its probability and would have given a wrong answer.
\end{example}

When a continuous distribution (say, Gamma) is approximated by another continuous distribution (Normal), the continuity correction is not needed. In fact, it would be an error to use it in this case because it would no longer preserve the probability.

